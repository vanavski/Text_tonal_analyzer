# Разбиение документа на N-граммы и подсчет весов
В данном файле описывается функционал классов `DocumentPreparer` и `TextWeightCounter`.

Класс `DocumentPreparer` занимается разбиением документа на N-граммы.

В компьютерной лингвистике существует два способа представления текста: **bag-of-words** (мешок слов) и **набор N-грамм**
(комбинации из N слов).\
Представление документа в виде bag-of-words выглядит следующим образом: \
`Text: Я люблю черный кофе` \
`Bag-of-Words: {"Я", "люблю", "черный", "кофе"}`

Как показывает опыт, **униграммы** (слова документа по отдельности), **биграммы** (комбинации из 2 слов) и **триграммы** 
(комбинации из 3 слов) показывают лучшие результаты при анализе тональности (в отличии от N-грамм более высоких порядков). \
Пример представления текста в виде набора биграмм и триграмм: \
`Text: Я люблю черный кофе` \
`Bigrams: {"Я люблю", "люблю черный", "черный кофе"}` \
`Trigrams: {"Я люблю черный", "люблю черный кофе"}`

После разбиения текста на N-граммы происходит выделение его признаков. Так как при классификации используется машинное
обучение, самыми удобными для классификатора признаками являются числа. Для получения данных признаков (весов документа)
используется формула <i>ΔTF-IDF</i>, которая учитывает, в скольки положительных и отрицательных документах встречается данная
N-грамма. Результатом ее работы является число, которое, грубо говоря, характеризует эмоциональную окраску N-граммы.

Такие вычисления выполняются для всех униграмм, биграмм и триграмм. После этого мы получаем три числа (вес по униграммам, биграммам
и триграммам соответственно), которые и являются признаками текста.

[Классификация тональности →](./count_tonal.md)

[← Лемматизация](./lemmatization.md)
